{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CLASSIFICATION OF SENTENCESS BASED ON Universal-sentence-encoder and keras modelling #########\n",
    "## https://www.dlology.com/blog/keras-meets-universal-sentence-encoder-transfer-learning-for-text-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I need daily rations in Attur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Give rice  dal oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>people are hungary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>provide rations for 6 people for 10 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Need prepared food for 20 people in slum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                      text\n",
       "0     0             I need daily rations in Attur\n",
       "1     0                        Give rice  dal oil\n",
       "2     0                       people are hungary \n",
       "3     0  provide rations for 6 people for 10 days\n",
       "4     0  Need prepared food for 20 people in slum"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataframe(filename):\n",
    "    lines = open(filename, 'r').read().splitlines()\n",
    "    data = []\n",
    "    for i in range(0, len(lines)):\n",
    "        label = lines[i].split(' ')[0]\n",
    "        label = label.split(\",\")[0]\n",
    "        text = ' '.join(lines[i].split(',')[1:])\n",
    "        text = re.sub(',','', text)\n",
    "        data.append([label, text])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
    "    df.label = df.label.astype('category')\n",
    "    return df\n",
    "\n",
    "df_train = get_dataframe('sentences.txt')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list of one-hot encoded labels\n",
    "train_text = df_train['text'].tolist()\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "train_label = np.asarray(pd.get_dummies(df_train.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = len(df_train.label.cat.categories)\n",
    "module_url = \"tf_sent_encoder_2_MODEL_PRETRAINED\" \n",
    "embed = hub.Module(module_url)\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)),signature=\"default\", as_dict=True)[\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0504 14:34:07.140520 20892 deprecation_wrapper.py:119] From c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0504 14:34:07.143542 20892 deprecation_wrapper.py:119] From c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0504 14:34:09.596715 20892 deprecation_wrapper.py:119] From c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0504 14:34:09.793557 20892 deprecation_wrapper.py:119] From c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0504 14:34:09.841674 20892 deprecation_wrapper.py:119] From c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "embedding = Lambda(UniversalEmbedding,output_shape=(embed_size,))(input_text)\n",
    "dense1 = Dense(256, activation='relu')(embedding)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "pred = Dense(category_counts, activation='softmax')(dense2)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 164,869\n",
      "Trainable params: 164,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataframe('test.txt')\n",
    "### list of one-hot encoded labels\n",
    "test_text = df_test['text'].tolist()\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Need food for diabeties patient'],\n",
       "       ['I want bread and rice for my home'],\n",
       "       ['arm broken need to go to hospital'],\n",
       "       ['food poisoning by eating old distributed food'],\n",
       "       ['network tower broken down in whitefield'],\n",
       "       ['calls are not connecting from airtel'],\n",
       "       ['I am a doctor  my car brokedown and want it to get repaired urgently.'],\n",
       "       ['My visa about to get expire. Want some travel arrangements to be done to fly back to Austia.'],\n",
       "       ['My neighbor attempted suicide .She is in hospital right now  urgent help needed'],\n",
       "       ['I saw bruises on my maid face. I think she is a victim of domestic violence.']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "20/40 [==============>...............] - ETA: 2s - loss: 1.6075 - acc: 0.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x000001BD7E89D828>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 88ms/step - loss: 1.6110 - acc: 0.1500 - val_loss: 1.5837 - val_acc: 0.4000\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.5484 - acc: 0.7250 - val_loss: 1.5432 - val_acc: 0.6000\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.4864 - acc: 0.9500 - val_loss: 1.4984 - val_acc: 0.7000\n",
      "Epoch 4/10\n",
      "20/40 [==============>...............] - ETA: 0s - loss: 1.4171 - acc: 0.9500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x000001BD7E89D828>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step - loss: 1.4202 - acc: 0.9750 - val_loss: 1.4440 - val_acc: 0.7000\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.3395 - acc: 0.9750 - val_loss: 1.3813 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.2457 - acc: 1.0000 - val_loss: 1.3108 - val_acc: 0.7000\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.1359 - acc: 1.0000 - val_loss: 1.2333 - val_acc: 0.7000\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 1.0181 - acc: 1.0000 - val_loss: 1.1511 - val_acc: 0.7000\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.8930 - acc: 1.0000 - val_loss: 1.0709 - val_acc: 0.7000\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.7621 - acc: 1.0000 - val_loss: 0.9899 - val_acc: 0.7000\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  history = model.fit(train_text, \n",
    "            train_label,\n",
    "            validation_data=(test_text, test_label),\n",
    "            epochs=10,batch_size=20)\n",
    "  model.save_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x000001BD7E89D828>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1473, in __del__\n",
      "    self._session._session, self._handle)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '4']\n"
     ]
    }
   ],
   "source": [
    "new_text = [\"wheat exhausted in hospital canteen\", \n",
    "            \"my neighbors house has crying voices. I think they need help\"]\n",
    "\n",
    "new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  model.load_weights('./model.h5')  \n",
    "  predicts = model.predict(new_text)\n",
    "\n",
    "categories = df_train.label.cat.categories.tolist()\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "predict_labels = [categories[logit] for logit in predict_logits]\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4154314 , 0.2754173 , 0.07836786, 0.13586688, 0.09491654],\n",
       "       [0.16206545, 0.1603227 , 0.15349114, 0.09350415, 0.43061662]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ With LARGE datasets ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['label','text'])\n",
    "trainset = sklearn.datasets.load_files(container_path = 'datalarge',encoding = 'UTF-8')\n",
    "\n",
    "def get_dataframe(lines):\n",
    "#     lines = filename.splitlines()\n",
    "    data = []\n",
    "    for i in range(0, len(lines)):\n",
    "        label = lines[i][0]\n",
    "    #     label = label.split(\",\")[0]\n",
    "        text = ' '.join(lines[i][1:])\n",
    "        text = re.sub(',','', text)\n",
    "        data.append([label, text])\n",
    "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
    "    df.label = df.label.astype('category')\n",
    "    return df\n",
    "\n",
    "for k in range(len(trainset.target)):\n",
    "    labelled_sent= trainset.data[k].strip().split('\\r\\n')\n",
    "    Label=[trainset.target[k]]*len(labelled_sent)\n",
    "    labelled_emb = list(zip([s for s in Label], [s for s in labelled_sent]))\n",
    "    df_train = get_dataframe(labelled_emb)\n",
    "    df=pd.concat([df,df_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, df['label'], test_size=0.2,random_state=42)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# X_train.reset_index(drop=True)\n",
    "# X_test.reset_index(drop=True)\n",
    "# y_train.reset_index(drop=True)\n",
    "# y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = np.array(X_train, dtype=object)[:,1][:,np.newaxis]\n",
    "train_label = np.asarray(pd.get_dummies(X_train.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = np.array(X_test, dtype=object)[:,1][:,np.newaxis]\n",
    "test_label = np.asarray(pd.get_dummies(X_test.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = len(np.unique(X_train.label))\n",
    "module_url = \"tf_sent_encoder_2_MODEL_PRETRAINED\" \n",
    "embed = hub.Module(module_url)\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)),signature=\"default\", as_dict=True)[\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "embedding = Lambda(UniversalEmbedding,output_shape=(embed_size,))(input_text)\n",
    "dense1 = Dense(256, activation='relu')(embedding)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "pred = Dense(category_counts, activation='softmax')(dense2)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3540 samples, validate on 885 samples\n",
      "Epoch 1/5\n",
      "3540/3540 [==============================] - 264s 75ms/step - loss: 0.8755 - acc: 0.6969 - val_loss: 0.2986 - val_acc: 0.9051\n",
      "Epoch 2/5\n",
      "3540/3540 [==============================] - 15s 4ms/step - loss: 0.2148 - acc: 0.9325 - val_loss: 0.1674 - val_acc: 0.9401\n",
      "Epoch 3/5\n",
      "3540/3540 [==============================] - 15s 4ms/step - loss: 0.1162 - acc: 0.9664 - val_loss: 0.1158 - val_acc: 0.9616\n",
      "Epoch 4/5\n",
      "3540/3540 [==============================] - 15s 4ms/step - loss: 0.0733 - acc: 0.9828 - val_loss: 0.0951 - val_acc: 0.9661\n",
      "Epoch 5/5\n",
      "3540/3540 [==============================] - 14s 4ms/step - loss: 0.0484 - acc: 0.9898 - val_loss: 0.0868 - val_acc: 0.9661\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  history = model.fit(train_text, \n",
    "            train_label,\n",
    "            validation_data=(test_text, test_label),epochs=5,batch_size=50)\n",
    "  model.save_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"wheat exhausted in hospital canteen\", \n",
    "            \"my neighbors house has crying voices. I think they need help\"]\n",
    "\n",
    "new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  model.load_weights('./model.h5')  \n",
    "  predicts = model.predict(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'abuse']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = np.unique(X_train.label)\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "predict_labels = [categories[logit] for logit in predict_logits]\n",
    "dict=['abuse','communication','food','fuel','health','travel']\n",
    "ANS=[dict[s] for s in predict_labels]\n",
    "print(ANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0. , 58.2,  0. , 41.8,  0. ],\n",
       "       [86.8, 11. ,  0. ,  0.2,  1.8,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(predicts*100,decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict=['abuse','communication','food','fuel','health','travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Combining both above cases ################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"wheat exhausted in hospital canteen and my neighbors house has crying voices. I think they need help\",\n",
    "           \"GIVE ME OIL\"]\n",
    "\n",
    "new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  model.load_weights('./model.h5')  \n",
    "  predicts = model.predict(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abuse', 'fuel']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = np.unique(X_train.label)\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "predict_labels = [categories[logit] for logit in predict_logits]\n",
    "dict=['abuse','communication','food','fuel','health','travel']\n",
    "ANS=[dict[s] for s in predict_labels]\n",
    "print(ANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63. ,  1.2,  6.9,  8.5, 18.8,  1.6],\n",
       "       [ 0. ,  0. ,  0.4, 99.5,  0.1,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(predicts*100,decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### TRY THIS - ANUP ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['label','text'])\n",
    "trainset = sklearn.datasets.load_files(container_path = 'datalarge',encoding = 'UTF-8')\n",
    "\n",
    "def get_dataframe(lines):\n",
    "#     lines = filename.splitlines()\n",
    "    data = []\n",
    "    for i in range(0, len(lines)):\n",
    "        label = lines[i][0]\n",
    "    #     label = label.split(\",\")[0]\n",
    "        text = ' '.join(lines[i][1:])\n",
    "        text = re.sub(',','', text)\n",
    "        data.append([label, text])\n",
    "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
    "    df.label = df.label.astype('category')\n",
    "    return df\n",
    "\n",
    "for k in range(len(trainset.target)):\n",
    "    labelled_sent= trainset.data[k].strip().split('\\r\\n')\n",
    "    Label=[trainset.target[k]]*len(labelled_sent)\n",
    "    labelled_emb = list(zip([s for s in Label], [s for s in labelled_sent]))\n",
    "    df_train = get_dataframe(labelled_emb)\n",
    "    df=pd.concat([df,df_train])\n",
    "    df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data=[]\n",
    "length=len(df)\n",
    "for i in range(length):\n",
    "    num1=random.randrange(length)\n",
    "    num2=random.randrange(length)\n",
    "    data.append([df.label[num1],df.label[num2],df.text[num1] + df.text[num2]])\n",
    "    \n",
    "df2 = pd.DataFrame(data, columns=['label1','label2', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df2, df2['label1'], test_size=0.2)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# X_train.reset_index(drop=True)\n",
    "# X_test.reset_index(drop=True)\n",
    "# y_train.reset_index(drop=True)\n",
    "# y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "main=[]\n",
    "for i in range(len(X_train)):\n",
    "    arr=np.array([X_train['label1'][i],X_train['label2'][i]])\n",
    "    main.append(arr)\n",
    "    \n",
    "# Create MultiLabelBinarizer object\n",
    "one_hot = MultiLabelBinarizer()\n",
    "\n",
    "# One-hot encode data\n",
    "train_text = np.array(X_train, dtype=object)[:,1][:,np.newaxis]\n",
    "train_label =np.asarray(one_hot.fit_transform(main), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "main=[]\n",
    "for i in range(len(X_test)):\n",
    "    arr=np.array([X_test['label1'][i],X_test['label2'][i]])\n",
    "    main.append(arr)\n",
    "    \n",
    "# Create MultiLabelBinarizer object\n",
    "one_hot = MultiLabelBinarizer()\n",
    "\n",
    "test_text = np.array(X_test, dtype=object)[:,1][:,np.newaxis]\n",
    "test_label = np.asarray(one_hot.fit_transform(main), dtype = np.int8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = len(np.unique(df.label))\n",
    "module_url = \"tf_sent_encoder_2_MODEL_PRETRAINED\" \n",
    "embed = hub.Module(module_url)\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)),signature=\"default\", as_dict=True)[\"default\"]\n",
    "\n",
    "# def sentence_encoder(input_text):\n",
    "#     return embed(tf.squeeze(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "embedding = Lambda(UniversalEmbedding,output_shape=(embed_size,))(input_text)\n",
    "dense1 = Dense(1024, activation='relu')(embedding)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "pred = Dense(category_counts, activation='sigmoid')(dense2)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 4 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Unable to get element as bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-48435d24e7c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m   history = model.fit(train_text, \n\u001b[0;32m     20\u001b[0m             \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             validation_data=(test_text, test_label))\n\u001b[0m\u001b[0;32m     22\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\i511977\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Unable to get element as bytes."
     ]
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#   K.set_session(session)\n",
    "#   session.run(tf.global_variables_initializer())\n",
    "#   session.run(tf.tables_initializer())\n",
    "#   history = model.fit(train_text, \n",
    "#             train_label,\n",
    "#             validation_data=(test_text, test_label),epochs=1)\n",
    "#   model.save_weights('./model.h5')\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  history = model.fit(train_text, \n",
    "            train_label,\n",
    "            validation_data=(test_text, test_label))\n",
    "  model.save_weights('./model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"No network connectivity in my area for ordering groceries. Need ration urgently\",\n",
    "\"Need gas cylinder as my mobile broke down.\",\n",
    "\"My father is sick. i want daily ration\",\n",
    "\"Yesterday i saw few people bullying a person from my locality. I think he needs help.i need cab as my car broke down to go to hospital.\"]\n",
    "\n",
    "new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "predicts = model.predict(new_text)\n",
    "dictt=['abuse','communication','food','fuel','health','travel']\n",
    "\n",
    "for j in range(len(new_text)):\n",
    "    print(\"QUERY \"+str(j) +'-> ' + new_text[j])\n",
    "    print('\\n')\n",
    "    for i in range(len(dictt)):\n",
    "        print('\\t'+dictt[i]+\":\"+str(np.around(predicts*100,decimals=1)[j][i]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}