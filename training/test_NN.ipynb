{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  label                                      text\n0     0             I need daily rations in Attur\n1     0                        Give rice  dal oil\n2     0                       people are hungary \n3     0  provide rations for 6 people for 10 days\n4     0  Need prepared food for 20 people in slum",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I need daily rations in Attur</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Give rice  dal oil</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>people are hungary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>provide rations for 6 people for 10 days</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Need prepared food for 20 people in slum</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "def get_dataframe(filename):\n",
    "    lines = open(filename, 'r').read().splitlines()\n",
    "    data = []\n",
    "    for i in range(0, len(lines)):\n",
    "        label = lines[i].split(' ')[0]\n",
    "        label = label.split(\",\")[0]\n",
    "        text = ' '.join(lines[i].split(',')[1:])\n",
    "        text = re.sub(',','', text)\n",
    "        data.append([label, text])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
    "    df.label = df.label.astype('category')\n",
    "    return df\n",
    "\n",
    "df_train = get_dataframe('sentences.txt')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list of one-hot encoded labels\n",
    "train_text = df_train['text'].tolist()\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "train_label = np.asarray(pd.get_dummies(df_train.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(40, 1)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = len(df_train.label.cat.categories)\n",
    "module_url = \"../tf_sent_encoder_2\" \n",
    "embed = hub.Module(module_url)\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "512"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)),signature=\"default\", as_dict=True)[\"default\"]\n",
    "\n",
    "def sentence_encoder(input_text):\n",
    "    return embed(tf.squeeze(input_text))\n",
    "\n",
    "# def UniversalEmbedding(x):\n",
    "#     import tensorflow as tf\n",
    "#     import tensorflow_hub as hub\n",
    "#     module_url = \"tf_sent_encoder_2_MODEL_PRETRAINED\" \n",
    "#     embed = hub.Module(module_url)\n",
    "# #     embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
    "#     return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
    }
   ],
   "source": [
    "\n",
    "input_text = Input(shape=(1,), dtype=tf.string)\n",
    "# input_text = Input(tf.placeholder(dtype=tf.string, shape=[None]))\n",
    "#embedding = embed(input_text)\n",
    "embedding = Lambda(sentence_encoder, output_shape=(embed_size,))(input_text)\n",
    "dense1 = Dense(256, activation='relu')(embedding)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "pred = Dense(category_counts, activation='softmax')(dense2)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_34 (InputLayer)        (None, 1)                 0         \n_________________________________________________________________\nlambda_23 (Lambda)           (None, 512)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 256)               131328    \n_________________________________________________________________\ndense_10 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndense_11 (Dense)             (None, 5)                 645       \n=================================================================\nTotal params: 164,869\nTrainable params: 164,869\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_dataframe('test.txt')\n",
    "### list of one-hot encoded labels\n",
    "test_text = df_test['text'].tolist()\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "test_label = np.asarray(pd.get_dummies(df_test.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Local\\Continuum\\anaconda3\\envs\\text_processing\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nTrain on 40 samples, validate on 10 samples\nEpoch 1/50\nWARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From C:\\Users\\i509787\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n40/40 [==============================] - 11s 267ms/step - loss: 1.5980 - acc: 0.4000 - val_loss: 1.5522 - val_acc: 0.6000\nEpoch 2/50\n40/40 [==============================] - 0s 2ms/step - loss: 1.5172 - acc: 0.9000 - val_loss: 1.5125 - val_acc: 0.6000\nEpoch 3/50\n40/40 [==============================] - 0s 1ms/step - loss: 1.4505 - acc: 0.9250 - val_loss: 1.4700 - val_acc: 0.7000\nEpoch 4/50\n40/40 [==============================] - 0s 1ms/step - loss: 1.3749 - acc: 0.9750 - val_loss: 1.4193 - val_acc: 0.7000\nEpoch 5/50\n40/40 [==============================] - 0s 1ms/step - loss: 1.2937 - acc: 1.0000 - val_loss: 1.3603 - val_acc: 0.7000\nEpoch 6/50\n40/40 [==============================] - 0s 2ms/step - loss: 1.1973 - acc: 1.0000 - val_loss: 1.2933 - val_acc: 0.7000\nEpoch 7/50\n40/40 [==============================] - 0s 1ms/step - loss: 1.0930 - acc: 1.0000 - val_loss: 1.2189 - val_acc: 0.7000\nEpoch 8/50\n40/40 [==============================] - 0s 3ms/step - loss: 0.9799 - acc: 1.0000 - val_loss: 1.1383 - val_acc: 0.7000\nEpoch 9/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.8592 - acc: 1.0000 - val_loss: 1.0606 - val_acc: 0.7000\nEpoch 10/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.7374 - acc: 1.0000 - val_loss: 0.9870 - val_acc: 0.7000\nEpoch 11/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.6200 - acc: 1.0000 - val_loss: 0.9166 - val_acc: 0.7000\nEpoch 12/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.5087 - acc: 1.0000 - val_loss: 0.8457 - val_acc: 0.8000\nEpoch 13/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.4095 - acc: 1.0000 - val_loss: 0.7812 - val_acc: 0.8000\nEpoch 14/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.3236 - acc: 1.0000 - val_loss: 0.7348 - val_acc: 0.8000\nEpoch 15/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.2504 - acc: 1.0000 - val_loss: 0.7065 - val_acc: 0.8000\nEpoch 16/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.1900 - acc: 1.0000 - val_loss: 0.6871 - val_acc: 0.8000\nEpoch 17/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.1456 - acc: 1.0000 - val_loss: 0.6728 - val_acc: 0.8000\nEpoch 18/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.1098 - acc: 1.0000 - val_loss: 0.6735 - val_acc: 0.8000\nEpoch 19/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0841 - acc: 1.0000 - val_loss: 0.6874 - val_acc: 0.8000\nEpoch 20/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0664 - acc: 1.0000 - val_loss: 0.7050 - val_acc: 0.8000\nEpoch 21/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0508 - acc: 1.0000 - val_loss: 0.7191 - val_acc: 0.8000\nEpoch 22/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.7360 - val_acc: 0.8000\nEpoch 23/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0306 - acc: 1.0000 - val_loss: 0.7528 - val_acc: 0.7000\nEpoch 24/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.7666 - val_acc: 0.7000\nEpoch 25/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.7775 - val_acc: 0.7000\nEpoch 26/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.7876 - val_acc: 0.7000\nEpoch 27/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.7977 - val_acc: 0.7000\nEpoch 28/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.8089 - val_acc: 0.7000\nEpoch 29/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.8190 - val_acc: 0.7000\nEpoch 30/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.7000\nEpoch 31/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.8474 - val_acc: 0.7000\nEpoch 32/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.8588 - val_acc: 0.7000\nEpoch 33/50\n32/40 [=======================>......] - ETA: 0s - loss: 0.0076 - acc: 1.00040/40 [==============================] - 0s 1ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.8663 - val_acc: 0.7000\nEpoch 34/50\n40/40 [==============================] - 0s 3ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.8732 - val_acc: 0.7000\nEpoch 35/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.8798 - val_acc: 0.7000\nEpoch 36/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.8845 - val_acc: 0.8000\nEpoch 37/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.8865 - val_acc: 0.8000\nEpoch 38/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.8856 - val_acc: 0.8000\nEpoch 39/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.8859 - val_acc: 0.8000\nEpoch 40/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.8867 - val_acc: 0.8000\nEpoch 41/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.8867 - val_acc: 0.8000\nEpoch 42/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.8845 - val_acc: 0.8000\nEpoch 43/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.8825 - val_acc: 0.8000\nEpoch 44/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.8835 - val_acc: 0.8000\nEpoch 45/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.8857 - val_acc: 0.8000\nEpoch 46/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.8888 - val_acc: 0.8000\nEpoch 47/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.8926 - val_acc: 0.8000\nEpoch 48/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.8957 - val_acc: 0.8000\nEpoch 49/50\n40/40 [==============================] - 0s 2ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.8986 - val_acc: 0.8000\nEpoch 50/50\n40/40 [==============================] - 0s 1ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.9009 - val_acc: 0.8000\n"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  history = model.fit(train_text, \n",
    "            train_label,\n",
    "            validation_data=(test_text, test_label),\n",
    "            epochs=50)\n",
    "  model.save_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "K.set_session(session)\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.tables_initializer())\n",
    "model.load_weights('./model.h5')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['1', '4']\n"
    }
   ],
   "source": [
    "new_text = [\"wheat exhausted in hospital canteen\", \n",
    "            \"I was abused badly\"]\n",
    "\n",
    "new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "predicts = model.predict(new_text)\n",
    "\n",
    "categories = df_train.label.cat.categories.tolist()\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "predict_labels = [categories[logit] for logit in predict_logits]\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['wheat exhausted in hospital canteen'],\n       ['I was abused badly']], dtype=object)"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 4], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "predict_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[3.0733538e-01, 6.8997800e-01, 1.0854426e-03, 5.8886816e-04,\n        1.0123923e-03],\n       [2.7305773e-04, 2.3288429e-03, 1.5826846e-03, 3.0432959e-04,\n        9.9551105e-01]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('text_processing': conda)",
   "language": "python",
   "name": "python37664bittextprocessingcondacdb2982750b1430d90ffe1db2903e3a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}